sentences=sent_tokenize(sample_blog)
sentences_clean=[re.sub(r'[^\W\s]','', sentence.lower()) for sentence in sentences]
stop_words = stopwords.words('english')


sentence_tokens =[[words for words in sentence.split(' ') if words not in stop_words] for sentence in sentences_clean]
w2v=Word2Vec(sentence_tokens, vector_size=1,min_count=1, epochs=1000)

sentence_embeddings=[[w2v.wv.get_vector(word)[0] for word in words] for words in sentence_tokens]

max_len=max([len(tokens) for tokens in sentence_tokens])

sentence_embeddings=[np.pad(embedding, (0, max_len-len (embedding)), 'constant') for embedding in sentence_embeddings]
